"""Run profile benchmarking and generate benchmarking graphics."""

import ast
import datetime
import pstats
import sys
import textwrap
from argparse import (
    ArgumentDefaultsHelpFormatter,
    ArgumentParser,
    RawDescriptionHelpFormatter,
)
from io import StringIO
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


def get_function_map(root: Path) -> dict[tuple[str, int, str], str]:
    """Create an AST function map.

    This function uses the AST parser to scan the pyrealm codebase and identify all of
    the function definitions that might be referenced within profiling stats. The AST
    nodes are then used to build a dictionary that maps call information from profiling
    - which uses line numbers to identify calls within source files - onto call ids from
    the package AST structure, will be more stable across versions.

    The dictionary is keyed using (source file name, line number, call name) and the
    values are strings that identify functions within source files and methods within
    classes within source files.

    One added difficulty is that the line numbering differs betwen the two approaches.
    The line numbers reported by `ast` nodes are the _actual_ line where the `def` or
    `class` statement is found. The line numbers reported by `pstats` are offset if the
    callable has decorators, so this mapping function has to find the lowest line number
    of the function node and any decorator nodes.

    Warning:
        In order to separate class methods from functions, this function relies on
        `ast.walk` walking `ast.ClassDef` nodes before `ast.FunctionDef` nodes and then
        adds a `class_name` attribute to functions in the class body. That attribute can
        then be checked when the `ast.FunctionDef` node is revisited later in the walk.
        This is a bit of a hack.

    Returns:
        A dictionary mapping profile call ids to AST based call id.
    """

    ast_map = {}

    # Visit all the python modules below the root and add FunctionDef details to the
    # ast_map list. This relies on a hack - ClassDef are walked before FunctionDef and
    # so methods can be discovered and annotated with the class_name attribute before
    # they are added to the ast_map. https://stackoverflow.com/questions/64225210
    for src_file in root.rglob("*.py"):
        parsed_ast = ast.parse(src_file.read_text())

        for node in ast.walk(parsed_ast):
            if isinstance(node, ast.FunctionDef):
                # Find the line number used by profiling, which includes any decorators
                lineno = min([d.lineno for d in node.decorator_list + [node]])

                if hasattr(node, "class_name"):
                    ast_map[(str(src_file), lineno, node.name)] = (
                        f"{src_file}:{node.class_name}"  # type: ignore [attr-defined]
                        f".{node.name}"
                    )
                else:
                    ast_map[(str(src_file), lineno, node.name)] = (
                        f"{src_file}:{node.name}"
                    )
            if isinstance(node, ast.ClassDef):
                # Add parent node
                for child in node.body:
                    if isinstance(child, ast.FunctionDef):
                        child.class_name = node.name  # type: ignore [attr-defined]

    return ast_map


def convert_and_filter_prof_file(
    prof_path: Path,
    label: str,
    exclude: list[str] = ["{.*}", "<.*>", "/lib/", "/tests/"],
) -> pd.DataFrame:
    """Convert profiling output to a standard data frame.

    The function reads the contents of a ``.prof`` file (typically
    ``prof/combined.prof``) generated by running the profiling test suite and returns
    the profiling data as a standardised `pandas.DataFrame`.

    The profiling results include a field 'filename:lineno(function)', which identifies
    each profiled code object. Many of these will be from functions outside of the
    `pyrealm` codebase and these are excluded using a list of regex patterns:

    * '/lib/' excludes standard and site packages,
    * '{.*}' excludes profiling of '{built-in ... } and similar,
    * '<.*>' excludes profiling of '<frozen ...>` and similar.
    * '/tests/' excludes the test functions and classes calling the pyrealm code.

    The remaining rows should show the performance of just the pyrealm codebase.

    Args:
        prof_path: Path to the profiling output.
        label: A string used to label a particular profiling run, typically a commit SHA
        exclude: A list of patterns used to exclude rows from the profiling stats.
    """

    # Import the profile data, write the stats report to a StringIO and seek the start
    # to allow the data to be read. The print_stats() explicitly does not filter for
    # 'pyrealm' because the string can be found in virtual environment paths and leads
    # to inconsistent behaviour across platforms
    sio = StringIO()
    p = pstats.Stats(str(prof_path), stream=sio)
    p.print_stats()
    sio.seek(0)

    # Consume lines from the report to find the header row
    header_found = False
    while not header_found:
        header = sio.readline()
        if "ncalls" in header:
            header_found = True

    # Set replacement non-duplicated headers
    column_names = [
        "ncalls",
        "tottime",
        "tottime_percall",
        "cumtime",
        "cumtime_percall",
        "filename:lineno(function)",
    ]

    # Convert to a DataFrame using fixed width format
    df = pd.read_fwf(sio, engine="python", names=column_names)

    # Reduce to rows not matching any of the regex exclude patterns
    exclude_rows = pd.DataFrame(
        [df["filename:lineno(function)"].str.contains(excl) for excl in exclude]
    ).any()
    df = df[~exclude_rows]

    # Add a timestamp from the file creation date
    m_time = datetime.datetime.fromtimestamp(prof_path.stat().st_mtime)
    df["timestamp"] = m_time.isoformat(timespec="seconds")

    # Split the filename, lineno, and function
    df[["filename", "lineno", "function"]] = df.pop(
        "filename:lineno(function)"
    ).str.extract(r"(.*):(.*?)\((.*)\)", expand=True)

    # The exclude patterns above should now have reduced the profiling data to only
    # calls within the pyrealm codebase, so shorten the filenames to their position
    # within the package by splitting on the string 'pyrealm' and keeping the last
    # entry. The leading pyrealm is replaced to make for easier matching with the AST
    # structure.
    df["filename"] = (
        df["filename"].str.split("pyrealm").apply(lambda x: f"pyrealm{x[-1]}")
    )

    # Map the profiling data onto the AST structure for the package
    ast_map = get_function_map(Path("pyrealm"))

    # Use the profile data to look up the AST callable id
    df["process_id"] = df.apply(
        lambda rw: ast_map[(rw.filename, int(rw.lineno), rw.function)], axis=1
    )

    # Add the provided git commit SHA for information
    df["label"] = label

    # Add fields to ignore particular benchmarks and allow regression
    df["ignore_result"] = False
    df["ignore_justification"] = ""

    return df


def run_benchmark(
    incoming: pd.DataFrame,
    database_path: Path,
    fail_data_path: Path,
    plot_path: Path | None = None,
    tolerance: float = 0.05,
    n_runs: int = 5,
    update_on_pass: bool = False,
) -> bool:
    """Run benchmark checks on profiling results.

    This function takes new profiling results and benchmarks key performance indicators
    against the best results from the N most recent runs in the profiling database. The
    incoming data must have all performance indicators within the given tolerance of the
    fastest of the past N runs for all functionality that is present in both performance
    datasets.

    If any functionality fails the performance testing, the failing functions are
    written to data file along with the observed ('incoming') and target timings. If the
    performance benchmarking passes, the database can optionally be updated to include
    the incoming performance data. This is typically only be used when a pull request is
    merged into the develop or main branches, to extend the test database ready for the
    next benchmarking run.

    Args:
        incoming: A data frame of incoming profiling data.
        database_path: The path to the database of previous profiling data
        fail_data_path: A path to write out data on functions that fail profiling
        plot_path: An optional path for creating a benchmarking plot.
        tolerance: Fractional acceptable change in performance
        n_runs: The number of most recent runs to use
        update_on_pass: Should the incoming data be added to the database.
        new_database: Should the incoming data be used to create a new profiling
            database file.
    """

    if not database_path.exists():
        print(f"Creating new database: {database_path}")
        incoming.to_csv(database_path, index=False)
        return True

    # Read database and reduce to most recent n runs.
    database = pd.read_csv(database_path)
    n_recent_timestamps = sorted(database["timestamp"].unique())[-n_runs:]
    database = database.loc[database["timestamp"].isin(n_recent_timestamps)]

    # Find the best (minimum) previous indicator values for each label from those runs
    # in the database that have not explicitly been ignored.  Note that min() here takes
    # the minimum value for each column within the group.
    kpis = ["tottime_percall", "cumtime_percall", "tottime", "cumtime"]
    targets = (
        database.loc[~database["ignore_result"], ["process_id"] + kpis]
        .groupby("process_id")
        .min()
    )
    targets.columns = targets.columns + "_target"

    # Combine the incoming and database profiling and merge with targets on label to get
    # the observed and target values for each function
    incoming_sha = incoming["label"].unique()[0]
    combined = pd.concat([database, incoming])
    combined["is_incoming"] = combined["label"] == incoming_sha
    combined = combined.merge(targets, how="left", on="process_id")

    # Calculate the relative KPIs for each kpi
    # TODO - handle zeros
    for kpi in kpis:
        combined["relative_" + kpi] = combined[kpi] / combined[kpi + "_target"]

    # Find rows where performance change of incoming profiling is outside tolerance
    threshold = 1 + tolerance
    kpi_fail = [combined[f"relative_{kpi}"] > threshold for kpi in kpis]
    failing_rows = combined[pd.DataFrame(kpi_fail).any() & combined["is_incoming"]]

    # Plot before handling failure
    if plot_path is not None:
        create_benchmark_plot(
            plot_path=plot_path, combined=combined, threshold=threshold
        )

    # If any rows fail then save out the rows for all results on failing labels and
    # return False
    if not failing_rows.empty:
        failure_info = combined[combined["process_id"].isin(failing_rows["process_id"])]
        failure_info.to_csv(fail_data_path, index=False)
        return False

    # Should the
    if update_on_pass:
        # Drop the fields added by this function and save the updated combined data
        combined.drop(
            columns=["is_incoming"]
            + [f"relative_{kpi}" for kpi in kpis]
            + [f"{kpi}_target" for kpi in kpis],
            inplace=True,
        )
        combined.to_csv(database_path, index=False)

    return True


def create_benchmark_plot(
    plot_path: Path,
    combined: pd.DataFrame,
    threshold: float = 1.05,
    kpi: str = "cumtime_percall",
) -> None:
    """Plot the benchmarking results.

    This function generates a PNG plot that plots the relative performance of each
    labelled code object. Performance is always relative to the fastest performance
    found in previous profiling runs and this is shown in square brackets alongside the
    label.

    The incoming performance is shown as circles and previous versions are shown as
    numbers. A vertical line indicates the threshold for failing the benchmarking.
    Previous values that have been explicitly set as `ignore_result` are shown in light
    grey for information.

    Args:
        plot_path: An output path for the plot
        combined: The combined profiling data to be plotted
        threshold: The upper threshold to pass benchmarking
        kpi: A string specifying the performance metric to plot
    """

    # Sort labels to display in order from most time consuming to least and split into
    # previous and incoming values.
    combined = combined.sort_values(by=f"{kpi}_target")
    incoming = combined[combined["is_incoming"]]
    database = combined[~combined["is_incoming"]]

    # Get the code version labels for the previous and incoming versions
    previous_versions = (
        database[["timestamp", "label"]]
        .drop_duplicates()
        .sort_values(by="timestamp", ascending=False)
    )
    incoming_version = incoming["label"].unique()[0]

    # A4 portrait
    plt.figure(figsize=(11.69, 8.27))

    # Plot each previous version using text 1...n as plot symbols, with lighter grey for
    # values that have been set as ignore_result.
    for idx, label in enumerate(previous_versions["label"]):
        subset = combined[combined["label"] == label]

        plt.scatter(
            subset[f"relative_{kpi}"],
            subset["process_id"]
            + subset[f"{kpi}_target"].map(lambda x: f" [{x:0.3f}]"),
            marker=f"${idx + 1}$",
            color=[
                "lightgray" if val else "dimgrey" for val in subset["ignore_result"]
            ],
            label=label,
        )

    # Plot the incoming data as open circles, blue points when inside tolerance and red
    # if outside tolerance.
    fail = [False if val <= threshold else True for val in incoming[f"relative_{kpi}"]]
    plt.scatter(
        incoming[f"relative_{kpi}"],
        incoming["process_id"]
        + incoming[f"{kpi}_target"].map(lambda x: f" [{x:0.3f}]"),
        marker="o",
        facecolors="none",
        color=["firebrick" if val else "royalblue" for val in fail],
        label=f"{incoming_version} [incoming]",
    )

    # Add a vertical line for the threshold value and a legend
    plt.axvline(threshold, linestyle="--", color="indianred", linewidth=0.3)
    plt.legend(loc="lower right", bbox_to_anchor=(1.0, 1.05), ncol=6, frameon=False)
    plt.xlabel(f"Relative {kpi}")
    plt.tight_layout()

    # Colour the labels of failing rows.
    for idx, val in enumerate(fail):
        if val:
            plt.gca().get_yticklabels()[idx].set_color("firebrick")

    # Save to file
    plt.savefig(plot_path, dpi=600)


def generate_test_database() -> None:
    """Function to create a test database.

    This function takes a single profiling output (`prof/combined.prof`) and duplicates
    the profiling data 5 times with some minor variation to create a local test database
    for checking the benchmarking processes.

    The function can be run from the package root at the command line using:

    python -c "from profiling.run_benchmarking import *; _generate_test_database()"
    """

    # Create testing database
    prof_path = Path("prof/combined.prof")
    df = convert_and_filter_prof_file(prof_path=prof_path, label="test2")
    copies: list = []

    for test in range(1, 6):
        df2 = df.copy()
        df2["label"] = f"test_{test}"
        df["timestamp"] = df["timestamp"].astype("datetime64[ns]") + np.timedelta64(
            1, "D"
        )
        kpis = ["tottime_percall", "cumtime_percall", "tottime", "cumtime"]

        for kpi in kpis:
            df2[kpi] = df[kpi] * np.random.uniform(
                low=0.98, high=1.03, size=len(df[kpi])
            )
        copies.append(df2)

    test_db = pd.concat(copies)
    test_db.to_csv("profiling/test-profiling-database.csv")


class RawDescAndDefaultsHelpFormatter(
    RawDescriptionHelpFormatter, ArgumentDefaultsHelpFormatter
):
    """Combine argparse formatter helpers and restrict width."""

    def __init__(self, prog: str) -> None:
        RawDescriptionHelpFormatter.__init__(self, prog, width=88, max_help_position=30)


def run_benchmarking_cli() -> None:
    """Run the package benchmarking.

    This function runs the standard benchmarking for the pyrealm package. The profiling
    tests in the test suite generate a set of combined profile data across the package
    functionality. This command then reads in a set of combined profile data and
    compares it to previous benchmark data.

    The profiling excludes all profiled code objects matching regex patterns provided
    using the `--exclude` argument. The defaults exclude standard and site packages,
    built in code and various other standard code, and are intended to reduce the
    benchmarking to only code objects within the package.
    """

    if run_benchmarking_cli.__doc__ is not None:
        doc = "    " + run_benchmarking_cli.__doc__
    else:
        doc = "Python in -OO mode"

    parser = ArgumentParser(
        description=textwrap.dedent(doc),
        formatter_class=RawDescAndDefaultsHelpFormatter,
    )
    parser.add_argument(
        "prof_path",
        type=Path,
        help="Path to pytest-profiling output",
    )
    parser.add_argument(
        "database_path",
        type=Path,
        help="Path to benchmarking database",
    )
    parser.add_argument(
        "fail_data_path",
        type=Path,
        help="Output path for data on benchmark fails",
    )
    parser.add_argument(
        "label",
        help="A text label for the incoming profiling results, typically a commit SHA",
    )
    parser.add_argument(
        "--exclude",
        action="append",
        help="Exclude profiled code matching a regex pattern, can be repeated",
        default=["{.*}", "<.*>", "/lib/", "/tests/"],
    )
    parser.add_argument(
        "--n-runs",
        help="Number of most recent runs to use in benchmarking",
        type=int,
        default=5,
    )
    parser.add_argument(
        "--tolerance",
        help="Tolerance of time cost increase in benchmarking",
        type=float,
        default=0.05,
    )
    parser.add_argument(
        "--update-on-pass",
        help="Update the profiling database if benchmarking passes",
        action="store_true",
    )
    parser.add_argument(
        "--plot-path", help="Generate a benchmarking plot to this path", type=Path
    )

    args = parser.parse_args()

    # Copy the profiling results to the current folder
    if not args.prof_path.exists():
        raise FileNotFoundError(f"Cannot find the profiling file at {args.prof_path}.")

    incoming = convert_and_filter_prof_file(
        prof_path=args.prof_path,
        label=args.label,
        exclude=args.exclude,
    )

    success = run_benchmark(
        incoming=incoming,
        database_path=args.database_path,
        fail_data_path=args.fail_data_path,
        tolerance=args.tolerance,
        n_runs=args.n_runs,
        update_on_pass=args.update_on_pass,
        plot_path=args.plot_path,
    )

    if not success:
        print("Benchmarking failed.")
        sys.exit(1)

    print("Benchmarking passed.")
    sys.exit(0)


if __name__ == "__main__":
    run_benchmarking_cli()
