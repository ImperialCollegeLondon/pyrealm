"""Run profile benchmarking and generate benchmarking graphics."""

import datetime
import pstats
import sys
import textwrap
from argparse import (
    ArgumentDefaultsHelpFormatter,
    ArgumentParser,
    RawDescriptionHelpFormatter,
)
from io import StringIO
from pathlib import Path

import matplotlib.pyplot as plt
import pandas as pd


def convert_and_filter_prof_file(
    prof_path: Path,
    commit_sha: str,
    exclude: list[str] = ["{.*}", "<.*>", "/lib/"],
) -> pd.DataFrame:
    """Convert profiling output to a standard data frame.

    The function reads the contents of a ``.prof`` file (typically
    ``prof/combined.prof``) generated by running the profiling test suite and returns
    the profiling data as a standardised `pandas.DataFrame`.

    The profiling results include a field 'filename:lineno(function)', which identifies
    each profiled code object. Many of these will be from functions outside of the
    `pyrealm` codebase and these are excluded using a list of regex patterns:

    * '/lib/' excludes standard and site packages,
    * '{.*}' excludes profiling of '{built-in ... } and similar,
    * '<.*>' excludes profiling of '<frozen ...>` and similar.

    The remaining rows should show the performance of just the pyrealm code.

    Args:
        prof_path: Path to the profiling output.
        commit_sha: A commit SHA used to label a particular profiling run.
        exclude: A list of patterns used to exclude rows from the profiling stats.
    """

    # Import the profile data, write the stats report to a StringIO and seek the start
    # to allow the data to be read. The print_stats() explicitly does not filter for
    # 'pyrealm' because the string can be found in virtual environment paths and leads
    # to inconsistent behaviour across platforms
    sio = StringIO()
    p = pstats.Stats(str(prof_path), stream=sio)
    p.print_stats()
    sio.seek(0)

    # Consume lines from the report to find the header row
    header_found = False
    while not header_found:
        header = sio.readline()
        if "ncalls" in header:
            header_found = True

    # Set replacement non-duplicated headers
    column_names = [
        "ncalls",
        "tottime",
        "tottime_percall",
        "cumtime",
        "cumtime_percall",
        "filename:lineno(function)",
    ]

    # Convert to a DataFrame using fixed width format
    df = pd.read_fwf(sio, engine="python", names=column_names)

    # Reduce to rows not matching any of the regex exclude patterns
    exclude_rows = pd.DataFrame(
        [df["filename:lineno(function)"].str.contains(excl) for excl in exclude]
    ).any()
    df = df[~exclude_rows]

    # Add a timestamp from the file creation date
    m_time = datetime.datetime.fromtimestamp(prof_path.stat().st_mtime)
    df["timestamp"] = m_time.isoformat(timespec="seconds")

    # Split the filename, lineno, and function
    df[["filename", "lineno", "function"]] = df.pop(
        "filename:lineno(function)"
    ).str.extract(r"(.*):(.*?)\((.*)\)", expand=True)

    # Get the unique part of the file paths - note that the zip won't cover all of the
    # parts as they are not of even length, but should cover the identical portions.
    parts = [Path(fn).parts for fn in df["filename"]]
    diverge_index = [len(set(pt)) == 1 for pt in zip(*parts)].index(False)
    df["filename"] = ["/".join(list(fn)[diverge_index:]) for fn in parts]

    # Add a unique label for each function for plotting
    df["label"] = df["filename"] + ":" + df["function"]

    # Add the provided git commit SHA for information
    df["commit_sha"] = commit_sha

    # Add fields to ignore particular benchmarks and allow regression
    df["ignore_result"] = False
    df["ignore_justification"] = ""

    return df


def run_benchmark(
    incoming: pd.DataFrame,
    database_path: Path,
    fail_data_path: Path,
    plot_path: Path | None = None,
    tolerance: float = 0.05,
    n_runs: int = 5,
    new_database: bool = False,
    append_on_pass: bool = False,
) -> bool:
    """Benchmark profiling results.

    This function takes new profiling results and benchmarks key performance indicators
    against the best results from the N most recent runs in the profiling database. The
    incoming data must have all performance indicators within the given tolerance of the
    fastest of the past N runs for all functionality that is present in both performance
    datasets.

    If any functionality fails the performance testing, the failing functions are
    written to data file along with the observed ('incoming') and target timings. If the
    performance benchmarking passes, the incoming data can optionally be appended to the
    database. This is typically only be used when a pull request is merged into the
    develop or main branches, to extend the test database ready for the next
    benchmarking run.

    Args:
        incoming: A data frame of incoming profiling data.
        database_path: The path to the database of previous profiling data
        fail_data_path: A path to write out data on functions that fail profiling
        plot_path: An optional path for creating a benchmarking plot.
        tolerance: Fractional acceptable change in performance
        n_runs: The number of most recent runs to use
        append_on_pass: Should the incoming data be added to the database.
        new_database: Should the incoming data be used to create a new profiling
            database file.
    """

    if new_database:
        if database_path.exists():
            raise RuntimeError(
                f"Cannot overwrite existing database file: {database_path}"
            )

        incoming.to_csv(database_path, index=False)
        return True

    if not database_path.exists():
        raise FileNotFoundError(f"Database file not found: {database_path}")

    # Read database and reduce to most recent n runs.
    database = pd.read_csv(database_path)
    n_recent_timestamps = sorted(database["timestamp"].unique())[-n_runs:]
    database = database.loc[database["timestamp"].isin(n_recent_timestamps)]

    # Find the best (minimum) previous indicator values for each label from those runs
    # in the database that have not explicitly been ignored.  Note that min() here takes
    # the minimum value for each column within the group.
    kpis = ["tottime_percall", "cumtime_percall", "tottime", "cumtime"]
    targets = (
        database.loc[~database["ignore_result"], ["label"] + kpis]
        .groupby("label")
        .min()
    )
    targets.columns = targets.columns + "_target"

    # Combine the incoming and database profiling and merge with targets on label to get
    # the observed and target values for each function
    incoming_sha = incoming["commit_sha"].unique()[0]
    combined = pd.concat([database, incoming])
    combined["is_incoming"] = combined["commit_sha"] == incoming_sha
    combined = combined.merge(targets, how="left", on="label")

    # Calculate the relative KPIs for each kpi
    # TODO - handle zeros
    for kpi in kpis:
        combined["relative_" + kpi] = combined[kpi] / combined[kpi + "_target"]

    # Find rows where performance change of incoming profiling is outside tolerance
    threshold = 1 + tolerance
    kpi_fail = [combined[f"relative_{kpi}"] > threshold for kpi in kpis]
    failing_rows = combined[pd.DataFrame(kpi_fail).any() & combined["is_incoming"]]

    # Plot before handling failure
    if plot_path is not None:
        create_benchmark_plot(
            plot_path=plot_path, combined=combined, threshold=threshold
        )

    # If any rows fail then save out the rows for all results on failing labels and
    # return False
    if not failing_rows.empty:
        failure_info = combined[combined["label"].isin(failing_rows["label"])]
        failure_info.to_csv(fail_data_path, index=False)
        return False

    # Otherwise, save the combined database after dropping internal fields if requested
    # and then return True
    if append_on_pass:
        combined.drop(
            columns=["is_incoming"]
            + [f"relative_{kpi}" for kpi in kpis]
            + [f"{kpi}_target" for kpi in kpis],
            inplace=True,
        )
        combined.to_csv(database_path, index=False)

    return True


def create_benchmark_plot(
    plot_path: Path,
    combined: pd.DataFrame,
    threshold: float = 1.05,
    kpi: str = "cumtime",
) -> None:
    """Plot the benchmarking results.

    This function generates a PNG plot that plots the relative performance of each
    labelled code object. Performance is always relative to the fastest performance
    found in previous profiling runs and this is shown in square brackets alongside the
    label.

    The incoming performance is shown as circles and previous versions are shown as
    numbers. A vertical line indicates the threshold for failing the benchmarking.
    Previous values that have been explicitly set as `ignore_result` are shown in light
    grey for information.

    Args:
        plot_path: An output path for the plot
        combined: The combined profiling data to be plotted
        threshold: The upper threshold to pass benchmarking
        kpi: A string specifying the performance metric to plot
    """

    # Sort labels to display in order from most time consuming to least and split into
    # previous and incoming values.
    combined = combined.sort_values(by=f"{kpi}_target")
    incoming = combined[combined["is_incoming"]]
    database = combined[~combined["is_incoming"]]

    # Get commit SHA values
    previous_versions = (
        database[["timestamp", "commit_sha"]]
        .drop_duplicates()
        .sort_values(by="timestamp", ascending=False)
    )
    incoming_version = incoming["commit_sha"].unique()[0]

    # A4 portrait
    plt.figure(figsize=(8.27, 11.69))

    # Plot each previous version using text 1...n as plot symbols, with lighter grey for
    # values that have been set as ignore_result.
    for idx, sha in enumerate(previous_versions["commit_sha"]):
        subset = combined[combined["commit_sha"] == sha]

        plt.scatter(
            subset[f"relative_{kpi}"],
            subset["label"] + subset[f"{kpi}_target"].map(lambda x: f" [{x:0.3f}]"),
            marker=f"${idx + 1}$",
            color=[
                "lightgray" if val else "dimgrey" for val in subset["ignore_result"]
            ],
            label=sha,
        )

    # Plot the incoming data as open circles, blue points when inside tolerance and red
    # if outside tolerance.
    fail = [False if val <= threshold else True for val in incoming[f"relative_{kpi}"]]
    plt.scatter(
        incoming[f"relative_{kpi}"],
        incoming["label"] + incoming[f"{kpi}_target"].map(lambda x: f" [{x:0.3f}]"),
        marker="o",
        facecolors="none",
        color=["firebrick" if val else "royalblue" for val in fail],
        label=f"{incoming_version} [incoming]",
    )

    # Add a vertical line for the threshold value and a legend
    plt.axvline(threshold, linestyle="--", color="indianred", linewidth=0.3)
    plt.legend(loc="lower right", bbox_to_anchor=(1.0, 1.05), ncol=6, frameon=False)
    plt.xlabel(f"Relative {kpi}")
    plt.tight_layout()

    # Colour the labels of failing rows.
    for idx, val in enumerate(fail):
        if val:
            plt.gca().get_yticklabels()[idx].set_color("firebrick")

    # Save to file
    plt.savefig(plot_path)


class RawDescAndDefaultsHelpFormatter(
    RawDescriptionHelpFormatter, ArgumentDefaultsHelpFormatter
):
    """Combine argparse formatter helpers and restrict width."""

    def __init__(self, prog: str) -> None:
        RawDescriptionHelpFormatter.__init__(self, prog, width=88, max_help_position=30)


def profile_report_cli() -> None:
    """Run the package benchmarking.

    This function runs the standard benchmarking for the pyrealm package. The profiling
    tests in the test suite generate a set of combined profile data across the package
    functionality. This function can then reads in a set of combined profile data and
    compare it to previous benchmark data.

    The profiling excludes all profiled code objects matching regex patterns provided
    using the `--exclude` argument. The defaults exclude standard and site packages,
    built in code and various other standard code, and are intended to reduce the
    benchmarking to only code objects within the package.
    """

    if profile_report_cli.__doc__ is not None:
        doc = "    " + profile_report_cli.__doc__
    else:
        doc = "Python in -OO mode"

    parser = ArgumentParser(
        description=textwrap.dedent(doc),
        formatter_class=RawDescAndDefaultsHelpFormatter,
    )
    parser.add_argument(
        "prof_path",
        type=Path,
        help="Path to pytest-profiling output",
    )
    parser.add_argument(
        "database_path",
        type=Path,
        help="Path to benchmarking database",
    )
    parser.add_argument(
        "fail_data_path",
        type=Path,
        help="Output path for data on benchmark fails",
    )
    parser.add_argument("commit_sha", help="Github commit SHA")
    parser.add_argument(
        "--exclude",
        action="append",
        help="Exclude profiled code matching a regex pattern, can be repeated",
        default=["{.*}", "<.*>", "/lib/"],
    )
    parser.add_argument(
        "--n-runs",
        help="Number of most recent runs to use in benchmarking",
        type=int,
        default=5,
    )
    parser.add_argument(
        "--tolerance",
        help="Tolerance of time cost increase in benchmarking",
        type=float,
        default=0.05,
    )
    parser.add_argument(
        "--append-on-pass",
        help="Add incoming data to the database when benchmarking passes",
        action="store_true",
    )
    parser.add_argument(
        "--new-database",
        help="Use the incoming data to start a new profiling database",
        action="store_true",
    )
    parser.add_argument(
        "--plot-path", help="Generate a benchmarking plot to this path", type=Path
    )

    args = parser.parse_args()

    # Copy the profiling results to the current folder
    if not args.prof_path.exists():
        raise FileNotFoundError(f"Cannot find the profiling file at {args.prof_path}.")

    # if orig_graph_path.exists():
    #     os.system(f"cp {orig_graph_path} {graph_path}")
    # else:
    #     print(f"Cannot find the call graph at {orig_graph_path}.")

    incoming = convert_and_filter_prof_file(
        prof_path=args.prof_path,
        commit_sha=args.commit_sha,
        exclude=args.exclude,
    )

    success = run_benchmark(
        incoming=incoming,
        database_path=args.database_path,
        fail_data_path=args.fail_data_path,
        tolerance=args.tolerance,
        n_runs=args.n_runs,
        append_on_pass=args.append_on_pass,
        new_database=args.new_database,
        plot_path=args.plot_path,
    )

    if not success:
        print("Benchmarking failed.")
        sys.exit(1)

    print("Benchmarking passed.")
    sys.exit(0)


if __name__ == "__main__":
    profile_report_cli()
