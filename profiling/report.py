"""Run profile benchmarking and generate benchmarking graphics."""

import datetime
import pstats
import sys
import textwrap
from argparse import ArgumentParser, Namespace, RawDescriptionHelpFormatter
from io import StringIO
from pathlib import Path

import matplotlib.pyplot as plt
import pandas as pd


def read_prof_as_dataframe(
    prof_path: Path,
    commit_sha: str,
    exclude: list[str] = ["{.*}", "<.*>", "/lib/"],
) -> pd.DataFrame:
    """Convert profiling output to a standard data frame.

    The function reads the contents of a ``.prof`` file (typically
    ``prof/combined.prof``) generated by running the profiling test suite and returns
    the profiling data as a standardised `pandas.DataFrame`.

    The profiling results include a field 'filename:lineno(function)', which identifies
    each profiled code object. Many of these will be from functions outside of the
    `pyrealm` codebase and these are excluded using a list of regex patterns:

    * '/lib/' excludes standard and site packages,
    * '{.*}' excludes profiling of '{built-in ... } and similar,
    * '<.*>' excludes profiling of '<frozen ...>` and similar.

    The remaining rows should show the performance of pyrealm code.

    Args:
        prof_path: Path to the profiling output.
        commit_sha: A commit SHA used to label a particular profiling run.
        exclude: A list of patterns used to exclude rows from the profiling stats.
    """

    # Import the profile data, write the stats report to a StringIO and seek the start
    # to allow the data to be read. The print_stats() explicitly does not filter for
    # 'pyrealm' because the string can be found in virtual environment paths and leads
    # to inconsistent behaviour across platforms
    sio = StringIO()
    p = pstats.Stats(str(prof_path), stream=sio)
    # p.strip_dirs()
    p.print_stats()
    sio.seek(0)

    # Consume lines from the report to find the header row
    header_found = False
    while not header_found:
        header = sio.readline()
        if "ncalls" in header:
            header_found = True

    # Set replacement non-duplicated headers
    column_names = [
        "ncalls",
        "tottime",
        "tottime_percall",
        "cumtime",
        "cumtime_percall",
        "filename:lineno(function)",
    ]

    # Convert to a DataFrame using fixed width format
    df = pd.read_fwf(sio, engine="python", names=column_names)

    # Reduce to rows not matching any of the regex exclude patterns
    exclude_rows = pd.DataFrame(
        [df["filename:lineno(function)"].str.contains(excl) for excl in exclude]
    ).any()
    df = df[~exclude_rows]

    # Add a timestamp from the file creation date
    m_time = datetime.datetime.fromtimestamp(prof_path.stat().st_mtime)
    df["timestamp"] = m_time.isoformat(timespec="seconds")

    # Split the filename, lineno, and function
    df[["filename", "lineno", "function"]] = df.pop(
        "filename:lineno(function)"
    ).str.extract(r"(.*):(.*?)\((.*)\)", expand=True)

    # Get the unique part of the file paths - note that the parts are not of even
    # length, but the zip should catch the similar portions
    parts = [Path(fn).parts for fn in df["filename"]]
    diverge_index = [len(set(pt)) == 1 for pt in zip(*parts)].index(False)
    df["filename"] = ["/".join(list(fn)[diverge_index:]) for fn in parts]

    # Add a unique label for each function for plotting
    df["label"] = df["filename"] + ":" + df["function"]

    # Add the provided git commit SHA for information
    df["commit_sha"] = commit_sha

    return df


def plot_profiling(df: pd.DataFrame, cfg: Namespace) -> None:
    """Plot the profiling results.

    Args:
        df: Profiling report DataFrame.
        cfg: Configuration of the profiling report.
    """

    df.plot.barh(
        y=["tottime_percall", "cumtime_percall"], x="label", figsize=(20, 10)
    )  # type: ignore
    plt.ylabel("")
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.legend(loc="lower right")

    if cfg.show:
        plt.show()

    if cfg.prof_plot_path:
        plt.savefig(cfg.prof_plot_path)


def run_benchmark(
    incoming: pd.DataFrame,
    database_path: Path,
    fail_path: Path,
    tolerance: float = 1.05,
    n_runs: int = 5,
    append_on_pass: bool = False,
) -> bool:
    """Benchmark profiling results.

    This function takes new profiling results and benchmarks key performance indicators
    against the best results from the N most recent runs in the profiling database. The
    incoming data must have all performance indicators within the given tolerance of the
    fastest of the past N runs for all functionality that is present in both performance
    datasets.

    If any functionality fails the performance testing, the failing functions are
    written to data file along with the observed ('incoming') and target timings. If the
    performance benchmarking passes, the incoming data can optionally be appended to the
    database. This is typically only be used when a pull request is merged into the
    develop or main branches, to extend the test database ready for the next
    benchmarking run.

    Args:
        incoming: A data frame of incoming profiling data.
        database_path: The path to the database of previous profiling data
        fail_path: A path to write out data on functions that fail profiling
        tolerance: Fractional acceptable change in performance
        n_runs: The number of most recent runs to use
        append_on_pass: Should the incoming data be added to the database.
    """

    # Read database and reduce to most recent n runs.
    database = pd.read_csv(database_path)
    n_recent_timestamps = sorted(database["timestamp"].unique())[-n_runs:]
    database = database.loc[database["timestamp"].isin(n_recent_timestamps)]

    # Find the best (minimum) previous indicator values from those runs in the database
    # - Note that min() here takes the minimum value for each column within the group.
    kpis = ["tottime_percall", "cumtime_percall", "tottime", "cumtime"]
    targets = database[["label"] + kpis].groupby("label").min()

    # Merge to get the incoming and target values of KPIS between matching functions
    # - Note that the inner join is used to deliberately drop functions that no longer
    #   exist (not in incoming) or are new to the package (not in database).
    comparison = incoming.merge(
        targets, how="inner", on="label", suffixes=("_incoming", "_target")
    )

    # Calculate the relative performance
    # - TODO deal with divide by zero.
    for kpi in kpis:
        comparison[f"{kpi}_performance"] = (
            comparison[f"{kpi}_incoming"] / comparison[f"{kpi}_target"]
        )

    # Find rows where performance change is outside tolerance
    kpi_fail = [comparison[f"{kpi}_performance"] > (1 + tolerance) for kpi in kpis]
    failing_rows = comparison[pd.DataFrame(kpi_fail).any()]

    # If any rows fail then save out the information and return False
    if not failing_rows.empty:
        failing_rows.to_csv(fail_path, index=False)
        return False

    # Otherwise, append the new data to the database and return True
    if append_on_pass:
        combined = pd.concat([database, incoming])
        combined.to_csv(database_path, index=False)

    return True


def plot_benchmark(bm: pd.DataFrame, cfg: Namespace) -> None:
    """Plot the benchmark results and check the performance change.

    Args:
        bm: Benchmark DataFrame.
        cfg: Configuration of the profiling report.
    """

    # Plot benchmark results
    bm.T.plot.barh(figsize=(20, 10))
    plt.tight_layout()
    plt.legend(loc="lower right")

    if cfg.show:
        plt.show()

    if cfg.bm_plot_path:
        plt.savefig(cfg.bm_plot_path)


def profile_report_cli() -> None:
    """Run the package benchmarking.

    This function runs the standard benchmarking for the pyrealm package. The profiling
    tests in the test suite generate a set of combined profile data across the package
    functionality. This function can then reads in a set of combined profile data and
    compare it to previous benchmark data.

    The profiling excludes all profiled code objects matching regex patterns provided
    using the `--exclude` argument. The defaults exclude standard and site packages,
    built in code and various other standard code, and are intended to reduce the
    benchmarking to only code objects within the package.
    """

    if profile_report_cli.__doc__ is not None:
        doc = "    " + profile_report_cli.__doc__
    else:
        doc = "Python in -OO mode"

    parser = ArgumentParser(
        description=textwrap.dedent(doc),
        formatter_class=RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--prof-path",
        type=Path,
        help="Path to pytest-profiling output",
        default="../prof/combined.prof",
    )
    parser.add_argument("--commit-sha", help="Github commit SHA", default="none")
    parser.add_argument(
        "--exclude",
        action="append",
        help="Exclude profiled code matching a regex pattern, can be repeated",
        default=["{.*}", "<.*>", "/lib/"],
    )
    parser.add_argument(
        "--database-path",
        type=Path,
        help="Path to benchmarking database",
        default="prof-report.csv",
    )
    parser.add_argument(
        "--n-runs",
        help="Number of most recent runs to use in benchmarking",
        type=int,
        default=5,
    )
    parser.add_argument(
        "--tolerance",
        help="Tolerance of time cost increase in benchmarking",
        type=float,
        default=0.05,
    )
    parser.add_argument(
        "--fail-path",
        type=Path,
        help="Output path for data on benchmark fails",
        default="benchmark-fails.csv",
    )
    parser.add_argument(
        "--append-on-pass",
        help="Add incoming data to the database when benchmarking passes",
        action="store_true",
    )
    parser.add_argument(
        "--no-save", help="Do not save the profiling results", action="store_true"
    )
    parser.add_argument(
        "--show", help="Show the plots (which blocks the program)", action="store_true"
    )

    args = parser.parse_args()

    # Copy the profiling results to the current folder
    if not args.prof_path.exists():
        raise FileNotFoundError(f"Cannot find the profiling file at {args.prof_path}.")

    # if orig_graph_path.exists():
    #     os.system(f"cp {orig_graph_path} {graph_path}")
    # else:
    #     print(f"Cannot find the call graph at {orig_graph_path}.")

    incoming = read_prof_as_dataframe(
        prof_path=args.prof_path,
        commit_sha=args.commit_sha,
        exclude=args.exclude,
    )

    success = run_benchmark(
        incoming=incoming,
        database_path=args.database_path,
        fail_path=args.fail_path,
        tolerance=args.tolerance,
        n_runs=args.n_runs,
        append_on_pass=args.append_on_pass,
    )

    # plot_profiling(df, cfg)
    # plot_benchmark(bm, cfg)
    if not success:
        print("Benchmarking failed.")
        sys.exit(1)

    print("Benchmarking passed.")
    sys.exit(0)


if __name__ == "__main__":
    profile_report_cli()
