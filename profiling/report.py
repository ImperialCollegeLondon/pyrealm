"""Run profile benchmarking and generate benchmarking graphics."""

import datetime
import pstats
import sys
import textwrap
from argparse import ArgumentParser, Namespace, RawDescriptionHelpFormatter
from io import StringIO
from pathlib import Path

import matplotlib.pyplot as plt
import pandas as pd


def read_prof_as_dataframe(
    prof_path: Path,
    commit_sha: str,
    path_filter: None | str = None,
) -> pd.DataFrame:
    """Convert profiling output to a standard data frame.

    The function reads the contents of a ``.prof`` file (typically
    ``prof/combined.prof``) generated by running the profiling test suite and returns
    the profiling data as a standardised `pandas.DataFrame`.

    Args:
        prof_path: Path to the profiling output.
        commit_sha: A commit SHA used to label a particular profiling run.
        path_filter: Only profile results with filenames matching this filter will be
            retained.
    """

    # Import the profile data, write the stats report to a StringIO and seek the start
    # to allow the data to be read
    sio = StringIO()
    p = pstats.Stats(str(prof_path), stream=sio)
    # p.strip_dirs()
    p.print_stats()  # p.print_stats("pyrealm")  # <---- NOT SURE ABOUT THIS FILTER
    sio.seek(0)

    # Consume lines from the report to find the header row
    header_found = False
    while not header_found:
        header = sio.readline()
        if "ncalls" in header:
            header_found = True

    # Set replacement non-duplicated headers
    column_names = [
        "ncalls",
        "tottime",
        "tottime_percall",
        "cumtime",
        "cumtime_percall",
        "filename:lineno(function)",
    ]

    # Convert to a DataFrame using fixed width format
    df = pd.read_fwf(sio, engine="python", names=column_names)

    #     .sort_values(by="cumtime_percall", ascending=False)
    #     .query("cumtime > 0.01")  # drop insignificant functions
    #     .dropna()
    # )

    # Split the filename, lineno, and function
    df[["filename", "lineno", "function"]] = df.pop(
        "filename:lineno(function)"
    ).str.extract(r"(.*):(.*?)\((.*)\)", expand=True)

    # Add a timestamp from the file creation date
    m_time = datetime.datetime.fromtimestamp(prof_path.stat().st_mtime)
    df["timestamp"] = m_time.isoformat(timespec="seconds")

    # Filter the paths by a regex pattern (usually the module name)
    if path_filter:
        df = df[df.filename.str.contains(path_filter)]

    # # Remove the common prefix of the filenames
    # i_diff = next(i for i, cs in enumerate(zip(*df.filename)) if len(set(cs)) > 1)
    # df["filename"] = [s[i_diff:] for s in df.filename]

    # Add a unique label for each function for plotting
    df["label"] = (
        df["filename"].str.extract(r"(\w+)\.py").squeeze() + "." + df["function"]
    )

    # Add GitHub event name for information
    df["commit_sha"] = commit_sha

    return df


def plot_profiling(df: pd.DataFrame, cfg: Namespace) -> None:
    """Plot the profiling results.

    Args:
        df: Profiling report DataFrame.
        cfg: Configuration of the profiling report.
    """

    df.plot.barh(
        y=["tottime_percall", "cumtime_percall"], x="label", figsize=(20, 10)
    )  # type: ignore
    plt.ylabel("")
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.legend(loc="lower right")

    if cfg.show:
        plt.show()

    if cfg.prof_plot_path:
        plt.savefig(cfg.prof_plot_path)


def run_benchmark(
    incoming: pd.DataFrame,
    database_path: Path,
    fail_path: Path,
    tolerance: float = 1.05,
    n_runs: int = 5,
    append_on_pass: bool = False,
) -> bool:
    """Benchmark profiling results.

    This function takes new profiling results and benchmarks key performance indicators
    against the best results from the N most recent runs in the profiling database. The
    incoming data must have all performance indicators within the given tolerance of the
    fastest of the past N runs for all functionality that is present in both performance
    datasets.

    If any functionality fails the performance testing, the failing functions are
    written to data file along with the observed ('incoming') and target timings. If the
    performance benchmarking passes, the incoming data can optionally be appended to the
    database. This is typically only be used when a pull request is merged into the
    develop or main branches, to extend the test database ready for the next
    benchmarking run.

    Args:
        incoming: A data frame of incoming profiling data.
        database_path: The path to the database of previous profiling data
        fail_path: A path to write out data on functions that fail profiling
        tolerance: Fractional acceptable change in performance
        n_runs: The number of most recent runs to use
        append_on_pass: Should the incoming data be added to the database.
    """

    # Read database and reduce to most recent n runs.
    database = pd.read_csv(database_path)
    n_recent_timestamps = sorted(database["timestamp"].unique())[-n_runs:]
    database = database.loc[database["timestamp"].isin(n_recent_timestamps)]

    # Find the best (minimum) previous indicator values from those runs in the database
    # - Note that min() here takes the minimum value for each column within the group.
    kpis = ["tottime_percall", "cumtime_percall", "tottime", "cumtime"]
    targets = database[["label"] + kpis].groupby("label").min()

    # Merge to get the incoming and target values of KPIS between matching functions
    # - Note that the inner join is used to deliberately drop functions that no longer
    #   exist (not in incoming) or are new to the package (not in database).
    comparison = incoming.merge(
        targets, how="inner", on="label", suffixes=("_incoming", "_target")
    )

    # Calculate the relative performance
    # - TODO deal with divide by zero.
    for kpi in kpis:
        comparison[f"{kpi}_performance"] = (
            comparison[f"{kpi}_incoming"] / comparison[f"{kpi}_target"]
        )

    # Find rows where performance change is outside tolerance
    kpi_fail = [comparison[f"{kpi}_performance"] > (1 + tolerance) for kpi in kpis]
    failing_rows = comparison[pd.DataFrame(kpi_fail).any()]

    # If any rows fail then save out the information and return False
    if not failing_rows.empty:
        failing_rows.to_csv(fail_path, index=False)
        return False

    # Otherwise, append the new data to the database and return True
    if append_on_pass:
        combined = pd.concat([database, incoming])
        combined.to_csv(database_path, index=False)

    return True


def plot_benchmark(bm: pd.DataFrame, cfg: Namespace) -> None:
    """Plot the benchmark results and check the performance change.

    Args:
        bm: Benchmark DataFrame.
        cfg: Configuration of the profiling report.
    """

    # Plot benchmark results
    bm.T.plot.barh(figsize=(20, 10))
    plt.tight_layout()
    plt.legend(loc="lower right")

    if cfg.show:
        plt.show()

    if cfg.bm_plot_path:
        plt.savefig(cfg.bm_plot_path)


def profile_report_cli() -> None:
    """Run the package benchmarking.

    This function runs the standard benchmarking for the pyrealm package. The profiling
    tests in the test suite generate a set of combined profile data across the package
    functionality. This function can then reads in a set of combined profile data and
    compare it to previous benchmark data.
    """

    if profile_report_cli.__doc__ is not None:
        doc = "    " + profile_report_cli.__doc__
    else:
        doc = "Python in -OO mode"

    parser = ArgumentParser(
        description=textwrap.dedent(doc),
        formatter_class=RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--prof-path",
        type=Path,
        help="Path to pytest-profiling output",
        default="../prof/combined.prof",
    )
    parser.add_argument("--commit-sha", help="Github commit SHA", default="none")
    parser.add_argument(
        "--path-filter", help="Filter the paths by regex, e.g. 'splash', 'pmodel'"
    )
    parser.add_argument(
        "--database-path",
        type=Path,
        help="Path to benchmarking database",
        default="prof-report.csv",
    )
    parser.add_argument(
        "--n-runs",
        help="Number of most recent runs to use in benchmarking",
        type=int,
        default=5,
    )
    parser.add_argument(
        "--tolerance",
        help="Tolerance of time cost increase in benchmarking",
        type=float,
        default=0.05,
    )
    parser.add_argument(
        "--fail-path",
        type=Path,
        help="Output path for data on benchmark fails",
        default="benchmark-fails.csv",
    )
    parser.add_argument(
        "--append-on-pass",
        help="Add incoming data to the database when benchmarking passes",
        action="store_true",
    )
    parser.add_argument(
        "--no-save", help="Do not save the profiling results", action="store_true"
    )
    parser.add_argument(
        "--show", help="Show the plots (which blocks the program)", action="store_true"
    )

    args = parser.parse_args()

    # Copy the profiling results to the current folder
    if not args.prof_path.exists():
        raise FileNotFoundError(f"Cannot find the profiling file at {args.prof_path}.")

    # if orig_graph_path.exists():
    #     os.system(f"cp {orig_graph_path} {graph_path}")
    # else:
    #     print(f"Cannot find the call graph at {orig_graph_path}.")

    incoming = read_prof_as_dataframe(
        prof_path=args.prof_path,
        commit_sha=args.commit_sha,
        path_filter=args.path_filter,
    )

    success = run_benchmark(
        incoming=incoming,
        database_path=args.database_path,
        fail_path=args.fail_path,
        tolerance=args.tolerance,
        n_runs=args.n_runs,
        append_on_pass=args.append_on_pass,
    )

    # plot_profiling(df, cfg)
    # plot_benchmark(bm, cfg)
    if not success:
        print("Benchmarking failed.")
        sys.exit(1)

    print("Benchmarking passed.")
    sys.exit(0)


if __name__ == "__main__":
    profile_report_cli()
